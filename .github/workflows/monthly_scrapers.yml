name: Run Monthly Web Scrapers
on:
  schedule:
    - cron: '0 0 1 * *'  # Run at midnight on the 1st day of each month
  workflow_dispatch:
    inputs:
      scraper:
        description: 'Monthly scraper to run (leave empty to run all)'
        required: false
        default: ''
jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    env:
      FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}
      EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
          
      - name: Install Chrome and Selenium dependencies
        run: |
          # Update package list
          sudo apt-get update
          
          # Install wget if not present
          sudo apt-get install -y wget
          
          # Download and install Chrome
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          
          # Create symbolic link where Selenium expects Chrome
          sudo ln -sf /usr/bin/google-chrome-stable /usr/bin/google-chrome
          
          # Verify Chrome installation
          google-chrome --version
          which google-chrome
          
          # Install ChromeDriver using webdriver-manager will handle version matching
          
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install openpyxl requests beautifulsoup4 selenium selenium-stealth webdriver-manager send2trash
          pip install "lxml[html_clean]"
          pip install requests-html
          
      # Initialize requests-html Chromium (for scrapers that use it)
      - name: Initialize requests-html Chromium
        run: |
          python -c "
          from requests_html import HTMLSession
          import sys
          print('Initializing Chromium for requests-html...')
          try:
              session = HTMLSession()
              r = session.get('https://example.com')
              r.html.render(timeout=30)
              print('Chromium successfully initialized')
          except Exception as e:
              print(f'Chromium initialization: {e}')
              sys.exit(0)  # Don't fail the workflow
          "
          
      - name: Test FTP connection
        run: python test_ftp.py
          
      - name: Download files from FTP
        run: |
          if [ -n "${{ github.event.inputs.scraper }}" ]; then
            python download_files.py ${{ github.event.inputs.scraper }}
          else
            python download_files.py --type monthly
          fi
          
      - name: Run monthly scrapers
        timeout-minutes: 300
        run: |
          if [ -n "${{ github.event.inputs.scraper }}" ]; then
            # Run specific scraper using the helper script
            python run_single_scraper.py ${{ github.event.inputs.scraper }}
            exit_code=$?
            if [ $exit_code -ne 0 ]; then
              echo "Scraper failed with exit code $exit_code"
              exit $exit_code
            fi
          else
            # Run all monthly scrapers
            python run_scrapers.py --type monthly
          fi
      
      # Add the artifact upload step here
      - name: Upload files as artifacts (for debugging)
        uses: actions/upload-artifact@v4
        with:
          name: pricing-spreadsheets
          path: Pricing Spreadsheets/*.xlsx
        if: always()  # This ensures files are uploaded even if previous steps fail
        
      - name: Upload results to FTP
        run: |
          if [ -n "${{ github.event.inputs.scraper }}" ]; then
            python upload_files.py ${{ github.event.inputs.scraper }}
          else
            python upload_files.py --type monthly
          fi
