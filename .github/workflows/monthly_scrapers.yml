name: Run Monthly Web Scrapers (Chunked)
on:
  schedule:
    - cron: '0 0 1 * *'  # Run at midnight on the 1st day of each month
  workflow_dispatch:
    inputs:
      scraper:
        description: 'Monthly scraper to run (leave empty to run all)'
        required: false
        default: ''
      start_page:
        description: 'Starting page number (for resuming)'
        required: false
        default: '1'
      max_pages:
        description: 'Maximum pages to process in this run'
        required: false
        default: '500'

jobs:
  # First chunk - pages 1-500
  scrape_chunk1:
    runs-on: ubuntu-latest
    timeout-minutes: 350  # Just under 6 hours
    env:
      FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}
      EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
      START_PAGE: 1
      MAX_PAGES: 500
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
          
      - name: Install Chrome and Selenium dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          sudo ln -sf /usr/bin/google-chrome-stable /usr/bin/google-chrome
          google-chrome --version
          
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install openpyxl requests beautifulsoup4 selenium selenium-stealth webdriver-manager send2trash
          pip install "lxml[html_clean]"
          
      - name: Test FTP connection
        run: python test_ftp.py
          
      - name: Download files from FTP
        run: |
          if [ -n "${{ github.event.inputs.scraper }}" ]; then
            python download_files.py ${{ github.event.inputs.scraper }}
          else
            python download_files.py --type monthly
          fi
          
      - name: Run monthly scrapers - Chunk 1
        timeout-minutes: 340
        run: |
          if [ -n "${{ github.event.inputs.scraper }}" ]; then
            # Run with page limits
            python Belfield_Monthly_Modified.py ${{ github.event.inputs.scraper }} --start-page 1 --max-pages 500
          else
            python run_scrapers.py --type monthly --chunk 1
          fi
      
      - name: Upload files as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: pricing-spreadsheets-chunk1
          path: Pricing Spreadsheets/*.xlsx
        if: always()
        
      - name: Upload results to FTP
        run: |
          if [ -n "${{ github.event.inputs.scraper }}" ]; then
            python upload_files.py ${{ github.event.inputs.scraper }}
          else
            python upload_files.py --type monthly
          fi

  # Second chunk - pages 501-1000
  scrape_chunk2:
    needs: scrape_chunk1  # Run after chunk1 completes
    runs-on: ubuntu-latest
    timeout-minutes: 350
    env:
      FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}
      EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
      START_PAGE: 501
      MAX_PAGES: 500
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
          
      - name: Install Chrome and Selenium dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          sudo ln -sf /usr/bin/google-chrome-stable /usr/bin/google-chrome
          
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install openpyxl requests beautifulsoup4 selenium selenium-stealth webdriver-manager send2trash
          pip install "lxml[html_clean]"
          
      - name: Download latest files from FTP (with chunk1 updates)
        run: |
          if [ -n "${{ github.event.inputs.scraper }}" ]; then
            python download_files.py ${{ github.event.inputs.scraper }}
          else
            python download_files.py --type monthly
          fi
          
      - name: Run monthly scrapers - Chunk 2
        timeout-minutes: 340
        run: |
          if [ -n "${{ github.event.inputs.scraper }}" ]; then
            # Run with page limits
            python Belfield_Monthly_Modified.py ${{ github.event.inputs.scraper }} --start-page 501 --max-pages 500
          else
            python run_scrapers.py --type monthly --chunk 2
          fi
      
      - name: Upload files as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: pricing-spreadsheets-chunk2
          path: Pricing Spreadsheets/*.xlsx
        if: always()
        
      - name: Upload final results to FTP
        run: |
          if [ -n "${{ github.event.inputs.scraper }}" ]; then
            python upload_files.py ${{ github.event.inputs.scraper }}
          else
            python upload_files.py --type monthly
          fi
